{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4e489b1-97cf-4203-a6c8-b4bb9999235b",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9164a90a-050c-4b55-b65f-836a2c817a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import nltk\n",
    "import sys\n",
    "import gensim.utils as gensimUtils\n",
    "\n",
    "# Get our models - The package will take care of downloading the models automatically\n",
    "# For best performance: Muennighoff/SGPT-5.8B-weightedmean-nli-bitfit\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-nli-bitfit\")\n",
    "model = AutoModel.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-nli-bitfit\")\n",
    "# Deactivate Dropout (There is no dropout in the above models so it makes no difference here but other SGPT models may have dropout)\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a407b2b-144b-4b6d-879d-efeadcc3a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = [\"\"]\n",
    "\n",
    "ET=pd.read_csv('ETresp21question-no-image.csv', encoding='latin1') #get data from student responses set\n",
    "df = pd.DataFrame(ET)    #set ET as dataframe\n",
    "dfIdeal = pd.DataFrame({'Gsentences': ET.GA}) #define ideal answers for tokenization (good answers/Gans).\n",
    "dfIdeal['tokenized_sents'] = dfIdeal.apply(lambda row: nltk.word_tokenize(row['Gsentences']), axis=1)\n",
    "dfStudent = pd.DataFrame({'Ssentences': ET.SA}) #define student answers for tokenization.\n",
    "dfStudent['tokenized_sents'] = dfStudent.apply(lambda row: nltk.word_tokenize(row['Ssentences']), axis=1)\n",
    "Gans = dfIdeal['tokenized_sents'] ##renaming the 2 tokenized sent sets for ease.\n",
    "Sans = dfStudent['tokenized_sents']\n",
    "GansDict = (dfIdeal['tokenized_sents'].to_dict)  #dictionary for tokenized sents\n",
    "SansDict = (dfStudent['tokenized_sents'].to_dict)\n",
    "\n",
    "\n",
    "tokenizedGans = [(gensimUtils.simple_preprocess(i, deacc=True, min_len=1, max_len=14)) for i in ET.GA] #tokenize Gans and Sans for use in w2v, w2vB, and D2V models matching(LSA allows for unkown terms in tokenized strings. these other models do not.)\n",
    "tokenizedSans = [(gensimUtils.simple_preprocess(i, deacc=True, min_len=1, max_len=14)) for i in ET.SA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7be61-f6b3-412a-933b-abc855bd5c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "374d3e3b-44d0-43d7-bf69-81e05f7906cf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59ff335-348f-46a3-a2a0-1868a767e878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9379d008-2c0b-4f18-97dd-2b06a7ecd636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "949c70e6-aea2-47c1-a1e9-6a35ce005c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAlist = []\n",
    "for i in ET.GA[0:100]:\n",
    "    GAlist.append(i)\n",
    "    \n",
    "SAlist = []\n",
    "for i in ET.SA[0:100]:\n",
    "    SAlist.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45008512-1e5f-469a-8783-74b559e697b2",
   "metadata": {},
   "source": [
    "# sentence embeddings, attention masks, weighting and mean pooling for ideal answers (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e66c30-adda-41c8-aa4b-864e1c37a278",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokensGA = tokenizer(GAlist, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "    last_hidden_stateGA = model(**batch_tokensGA, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "# Get weights of shape [bs, seq_len, hid_dim]\n",
    "weightsGA = (\n",
    "    torch.arange(start=1, end=last_hidden_stateGA.shape[1] + 1)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(-1)\n",
    "    .expand(last_hidden_stateGA.size())\n",
    "    .float().to(last_hidden_stateGA.device)\n",
    ")\n",
    "\n",
    "# Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "input_mask_expandedGA = (\n",
    "    batch_tokensGA[\"attention_mask\"]\n",
    "    .unsqueeze(-1)\n",
    "    .expand(last_hidden_stateGA.size())\n",
    "    .float()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd809c1-ce8a-484d-8a0f-23f44c18ae4e",
   "metadata": {},
   "source": [
    "# encodings for ideal responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0fbae7c-47a5-4f83-9a01-a474329b5df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5642,  1.0927, -0.2866,  ..., -0.1318,  1.1017,  1.3367],\n",
      "        [ 0.5642,  1.0927, -0.2866,  ..., -0.1318,  1.1017,  1.3367],\n",
      "        [ 0.5642,  1.0927, -0.2866,  ..., -0.1318,  1.1017,  1.3367],\n",
      "        ...,\n",
      "        [-0.7614, -0.5094,  0.6846,  ...,  1.2404, -0.1497, -1.2174],\n",
      "        [-0.7614, -0.5094,  0.6846,  ...,  1.2404, -0.1497, -1.2174],\n",
      "        [-0.7614, -0.5094,  0.6846,  ...,  1.2404, -0.1497, -1.2174]])\n"
     ]
    }
   ],
   "source": [
    "# Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "sum_embeddingsGA2 = torch.sum(last_hidden_stateGA * input_mask_expandedGA * weightsGA, dim=1)\n",
    "sum_maskGA = torch.sum(input_mask_expandedGA * weightsGA, dim=1)\n",
    "\n",
    "embeddingsGA = sum_embeddingsGA2 / sum_maskGA\n",
    "print(embeddingsGA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27284f6-9514-4c79-9378-f774d52b59d4",
   "metadata": {},
   "source": [
    "# sentence embeddings, attention masks, weighting and mean pooling for user responses (SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4380a15-ff33-4b16-97bc-6907b1b9e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_tokensSA = tokenizer(SAlist, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Get the embeddings\n",
    "with torch.no_grad():\n",
    "    # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "    last_hidden_stateSA = model(**batch_tokensSA, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "# Get weights of shape [bs, seq_len, hid_dim]\n",
    "weightsSA = (\n",
    "    torch.arange(start=1, end=last_hidden_stateSA.shape[1] + 1)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(-1)\n",
    "    .expand(last_hidden_stateSA.size())\n",
    "    .float().to(last_hidden_stateSA.device)\n",
    ")\n",
    "\n",
    "# Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "input_mask_expandedSA = (\n",
    "    batch_tokensSA[\"attention_mask\"]\n",
    "    .unsqueeze(-1)\n",
    "    .expand(last_hidden_stateSA.size())\n",
    "    .float()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd46d81-8e2b-406b-aaa2-a6cc24ea5674",
   "metadata": {},
   "source": [
    "# encodings for user responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc5f4b16-5997-4327-80c8-d568f0bbe9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "sum_embeddingsSA2 = torch.sum(last_hidden_stateSA * input_mask_expandedSA * weightsSA, dim=1)\n",
    "sum_maskSA = torch.sum(input_mask_expandedSA * weightsSA, dim=1)\n",
    "\n",
    "embeddingsSA = sum_embeddingsSA2 / sum_maskSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a5e15a9-c28b-4093-b327-32395fd9f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"%s\" and \"%s\" is: %.3f 0.2322816550731659\n",
      "Cosine similarity between \"%s\" and \"%s\" is: %.3f 0.1253511607646942\n",
      "Cosine similarity between \"%s\" and \"%s\" is: %.3f 0.26908594369888306\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(embeddingsSA[0], embeddingsGA[0])\n",
    "cosine_sim_0_2 = 1 - cosine(embeddingsSA[1], embeddingsGA[1])\n",
    "cosine_sim_0_3 = 1 - cosine(embeddingsSA[2], embeddingsGA[2])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\", (cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\", (cosine_sim_0_2))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" ,(cosine_sim_0_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "25297fff-5a31-4477-aba1-ba9a9502a53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coslist = []\n",
    "i = 0\n",
    "for cos in embeddingsSA:\n",
    "    cossim = 1 - cosine(embeddingsSA[i], embeddingsGA[i])\n",
    "    i += 1\n",
    "    coslist.append(cossim)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "241cefee-8667-484e-8f08-b8d8a1ef171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.18029339611530304, 0.09258537739515305, 0.20805393159389496, 0.12740613520145416, 0.09008137881755829, 0.08356942981481552, 0.07445529103279114, 0.11589616537094116, 0.39289483428001404, 0.10761389136314392, 0.13867785036563873, 0.06663820147514343, 0.4401565492153168, 0.20936426520347595, 0.0769776776432991, 0.10902649164199829, 0.15036354959011078, 0.12199797481298447, 0.09387026727199554, 0.12973879277706146, 0.4500756561756134, 0.5779394507408142, 0.3491973280906677, 0.586405336856842, 0.5891994833946228, 0.6395396590232849, 0.5970253348350525, 0.7139629125595093, 0.6450480818748474, 0.6524220108985901, 0.5496478080749512, 0.73369300365448, 0.589399516582489, 0.5819704532623291, 0.3421902358531952, 0.25750303268432617, 0.5998838543891907, 0.1437651813030243, 0.6487117409706116, 0.2535786032676697, 0.07951926440000534, 0.3697644770145416, 0.5458250045776367, 0.18977750837802887, 0.0902184322476387, 0.08031296730041504, 0.5078833699226379, 0.02787715010344982, 0.24939756095409393, 0.34074869751930237, 0.28716468811035156, 0.1303219348192215, 0.2877918481826782, 0.42688655853271484, 0.1759362667798996, 0.06538952887058258, 0.14520245790481567, 0.4451272189617157, 0.42487889528274536, 0.4447340667247772, 0.3001302480697632, 0.5363110899925232, 0.32729583978652954, 0.5743035078048706, 0.5519599914550781, 0.38128647208213806, 0.03636700287461281, 0.40955033898353577, 0.38920512795448303, 0.4950745701789856, 0.3691198527812958, 0.4514506757259369, 0.4300124943256378, 0.15514715015888214, 0.350850373506546, 0.3788609504699707, 0.2618769705295563, 0.4451272189617157, 0.42487889528274536, 0.4447340667247772, 0.3001302480697632, 0.5363110899925232, 0.32729583978652954, 0.5519599914550781, 0.38128647208213806, 0.03636700287461281, 0.40955033898353577, 0.38920512795448303, 0.4950745701789856, 0.3691198527812958, 0.4514506757259369, 0.4300124943256378, 0.15514715015888214, 0.350850373506546, 0.3788609504699707, 0.2618769705295563, 0.4451272189617157, 0.42487889528274536, 0.4447340667247772, 0.3001302480697632]\n"
     ]
    }
   ],
   "source": [
    "print(coslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0429ab3-9e9d-42af-ac3e-4a72fc47c983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfa138e-dff2-4262-854e-5d0139ff9aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (XPython)",
   "language": "python",
   "name": "xpython"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
